{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from ROI_extraction import preprocess_image\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common_functions import HSV_features_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import export_text\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "\n",
    "def eval_XGBoost(X,y,k,max_depth,num_rounds,learning_rate=0.3, min_split_loss=0,min_child_weight=1,max_delta_step=0,subsample=1,show_tree=False):\n",
    "    # Initialize a list to store the accuracy scores\n",
    "    accuracy_scores = []\n",
    "    val_accuracy_scores = []\n",
    "    test_accuracy_scores = []\n",
    "    features_weights = []\n",
    "    confusion_matrixs = []\n",
    "    val_confusion_matrixs = []\n",
    "    test_confusion_matrixs = []\n",
    "    \n",
    "    # Nombre de plis pour la validation croisée k-fold\n",
    "    k = 4\n",
    "\n",
    "    # Créer une instance de StratifiedKFold avec k plis\n",
    "    skf = StratifiedKFold(n_splits=k)\n",
    "\n",
    "    for i, (train_index, val_test_index) in enumerate(skf.split(X, y)):\n",
    "        if(show_tree):\n",
    "            print(f\"Fold {i}:\")\n",
    "        # Spécifie les données d'entrainement\n",
    "        X_train = X.iloc[train_index]\n",
    "        y_train = y[train_index]\n",
    "\n",
    "        # Spécifie les données de validation et de test\n",
    "        skf2 = StratifiedKFold(n_splits=2)\n",
    "        test_index,val_index = next(skf2.split(X.iloc[val_test_index], y[val_test_index]))\n",
    "        X_val = X.iloc[val_test_index].iloc[val_index]\n",
    "        y_val = y[val_test_index][val_index]\n",
    "        X_test = X.iloc[val_test_index].iloc[test_index]\n",
    "        y_test = y[val_test_index][test_index]\n",
    "\n",
    "        # Créer l'objet DMatrix pour les données d'entraînement et de test\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dval = xgb.DMatrix(X_val, label=y_val)\n",
    "        dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "        \n",
    "        # Définir les paramètres du modèle XGBoost\n",
    "        params = {\n",
    "            'objective': 'multi:softmax',  # Fonction objective pour la classification binaire\n",
    "            'max_depth': max_depth,  # Profondeur maximale de chaque arbre\n",
    "            'num_class': 3,\n",
    "            'learning_rate': learning_rate,\n",
    "            'min_split_loss' : min_split_loss,\n",
    "            'min_child_weight': min_child_weight,\n",
    "            'max_delta_step':max_delta_step,\n",
    "            'subsample':subsample\n",
    "        }\n",
    "        \n",
    "        model = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "        # Prédiction sur les données de test\n",
    "        y_train_pred = model.predict(dtrain)\n",
    "        y_val_pred = model.predict(dval)\n",
    "        y_test_pred = model.predict(dtest)\n",
    "\n",
    "        # Calculate the accuracy score\n",
    "        accuracy = metrics.accuracy_score(y_train, y_train_pred)\n",
    "        val_accuracy = metrics.accuracy_score(y_val, y_val_pred)\n",
    "        test_accuracy = metrics.accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "        confusion_mat = confusion_matrix(y_train, y_train_pred)\n",
    "        confusion_mat_val = confusion_matrix(y_val, y_val_pred)\n",
    "        confusion_mat_test = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "        if(show_tree):\n",
    "            print(f'Fold {i} accuracy : {accuracy}, val accuracy :{val_accuracy}, test accuracy : {test_accuracy}')\n",
    "\n",
    "        # Append the accuracy score to the list\n",
    "        accuracy_scores.append(accuracy)\n",
    "        val_accuracy_scores.append(val_accuracy)\n",
    "        test_accuracy_scores.append(test_accuracy)\n",
    "        confusion_matrixs.append(confusion_mat)\n",
    "        val_confusion_matrixs.append(confusion_mat_val)\n",
    "        test_confusion_matrixs.append(confusion_mat_test)\n",
    "\n",
    "    # Compute the average accuracy across all folds\n",
    "    if(show_tree):\n",
    "        print('-------------------')\n",
    "        print(f\"Train accuracy : {np.mean(accuracy_scores)}\")\n",
    "        print(f\"Val accuracy : {np.mean(val_accuracy_scores)}\")\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(4, 3))\n",
    "        ax.set_xlabel('Prédiction')\n",
    "        ax.set_ylabel('Vraie étiquette')\n",
    "        ax.set_title('Matrice de confusion')\n",
    "        print('Average confusion matrix')\n",
    "        sns.heatmap(np.mean(confusion_matrixs, axis=0), annot=True, cmap=\"Blues\", ax=ax, fmt='.0f')\n",
    "        plt.show()\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(4, 3))\n",
    "        ax.set_xlabel('Prédiction')\n",
    "        ax.set_ylabel('Vraie étiquette')\n",
    "        ax.set_title('Matrice de confusion val')\n",
    "        print('Average val confusion matrix')\n",
    "        sns.heatmap(np.mean(val_confusion_matrixs, axis=0), annot=True, cmap=\"Blues\", ax=ax, fmt='.0f')\n",
    "        plt.show()\n",
    "\n",
    "    return np.mean(accuracy_scores),np.mean(val_accuracy_scores),np.mean(test_accuracy_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw\n",
    "X,y = HSV_features_generation('../images/raw_pollub/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:57:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[09:57:08] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[09:57:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "[09:57:10] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0fdc6d574b9c0d168-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0, 0.962097008422887, 0.9560702875399361)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_XGBoost(X,y,5,100,6,50,learning_rate=0.3, min_split_loss=0,min_child_weight=1,max_delta_step=0,subsample=1,show_tree=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROI\n",
    "X,y = HSV_features_generation('../images/ROI_pollub/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_master_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
