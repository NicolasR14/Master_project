{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten,Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "# from ROI_extraction import preprocess_image\n",
    "import cv2\n",
    "import os\n",
    "from ROI_extraction import DataGenerator\n",
    "# Set the path to dataset\n",
    "dataset_path = '../images/3regimes'\n",
    "\n",
    "ids = []\n",
    "labels = {}\n",
    "classes = {'excess':1,'normal':0,'insufficient':-1}\n",
    "for class_name in os.listdir(dataset_path):\n",
    "    class_path = os.path.join(dataset_path, class_name)\n",
    "    if os.path.isdir(class_path):\n",
    "        for filename in os.listdir(class_path) :\n",
    "            if filename.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                img_path = os.path.join(class_path, filename) \n",
    "                ids.append(img_path)\n",
    "                labels[img_path]=classes[class_name]\n",
    "\n",
    "# # Shuffle the list of tuples\n",
    "# random.shuffle(ids)\n",
    "\n",
    "# # Define the split ratio (e.g., 80% for training, 20% for validation)\n",
    "# split_ratio = 0.8\n",
    "\n",
    "# # Calculate the index for splitting\n",
    "# split_index = int(len(ids) * split_ratio)\n",
    "\n",
    "# # Split the shuffled IDs and labels into training and validation sets\n",
    "# train_ids = ids[:split_index]\n",
    "# val_ids = ids[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_gamma(image):\n",
    "    # Convert image to float and normalize to range 0-1\n",
    "    image_normalized = image.astype(float) / 255.0\n",
    "\n",
    "    # Calculate mean R intensity\n",
    "    meanRimg = np.mean(image_normalized[:, :, 2])  # Image is in BGR format\n",
    "    \n",
    "    # Calculate G value\n",
    "    G = 0.74 * np.exp(-3.97 * meanRimg)\n",
    "    \n",
    "    # Apply transformation\n",
    "    transformed_image = np.power(image_normalized, 1 / G)\n",
    "    img_float32 = np.float32(transformed_image)\n",
    "    return img_float32\n",
    "\n",
    "def extract_ROI(original_image):\n",
    "    # Convert to grayscale\n",
    "    gray_image = cv2.cvtColor((original_image*255).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # # Apply histogram normalization\n",
    "    # normalized_image = cv2.equalizeHist(gray_image)\n",
    "    \n",
    "    # Apply median filtering\n",
    "    filtered_image = cv2.medianBlur(gray_image, 5)\n",
    "    \n",
    "    # Apply Otsu's thresholding\n",
    "    _, thresholded_image = cv2.threshold(filtered_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Apply morphological operations\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n",
    "    closed_image = cv2.morphologyEx(thresholded_image, cv2.MORPH_CLOSE, kernel)\n",
    "    opened_image = cv2.morphologyEx(closed_image, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    # Find contours in the processed image\n",
    "    contours, _ = cv2.findContours(opened_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Find the contour with the largest area\n",
    "    contour = max(contours, key=cv2.contourArea)\n",
    "    \n",
    "    # Calculate the moments of the contour\n",
    "    M = cv2.moments(contour)\n",
    "    \n",
    "    # Calculate the center of the contour\n",
    "    center_x = int(M[\"m10\"] / M[\"m00\"])\n",
    "    center_y = int(M[\"m01\"] / M[\"m00\"])\n",
    "    \n",
    "    # Calculate the coordinates of the square ROI\n",
    "    roi_size = 100\n",
    "    roi_x = center_x - roi_size // 2\n",
    "    roi_y = center_y - roi_size // 2\n",
    "    \n",
    "    return {'contours':contours,'roi_x':roi_x,'roi_y':roi_y,'roi_size':roi_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input image dimensions\n",
    "img_width, img_height = 100, 100\n",
    "n_channels = 3\n",
    "\n",
    "params = {'dim': (img_height,img_width),\n",
    "          'batch_size': 64,\n",
    "          'n_classes': 3,\n",
    "          'n_channels': n_channels,\n",
    "          'shuffle': False}\n",
    "\n",
    "# Set the number of classes\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generation(list_IDs_temp):\n",
    "    X = []\n",
    "    y = np.empty((len(list_IDs_temp)), dtype=int)\n",
    "    for i, ID in enumerate(list_IDs_temp):\n",
    "        image = cv2.imread(ID)\n",
    "        img_gamma_correct = correct_gamma(image)\n",
    "        ROI = extract_ROI(img_gamma_correct)\n",
    "        ROI = image[ROI['roi_y']:ROI['roi_y']+ROI['roi_size'], ROI['roi_x']:ROI['roi_x']+ROI['roi_size']]\n",
    "        # ROI = cv2.cvtColor(ROI, cv2.COLOR_BGR2GRAY)\n",
    "        X.append(ROI)\n",
    "        # Store class\n",
    "        y[i] = labels[ID]\n",
    "    X = np.reshape(X,(len(list_IDs_temp),img_width, img_height,params['n_channels']))\n",
    "    X = X.astype(\"float32\") / 255.0\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = data_generation(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Nombre de plis pour la validation croisée k-fold\n",
    "k = 5\n",
    "\n",
    "# Créer une instance de StratifiedKFold avec k plis\n",
    "skf = StratifiedKFold(n_splits=k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste pour stocker les scores de validation\n",
    "scores = {i:{'history':None,'history_fine_tuning':None} for i in range(k)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'history': None, 'history_fine_tuning': None},\n",
       " 1: {'history': None, 'history_fine_tuning': None},\n",
       " 2: {'history': None, 'history_fine_tuning': None},\n",
       " 3: {'history': None, 'history_fine_tuning': None},\n",
       " 4: {'history': None, 'history_fine_tuning': None}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{-1: 1670, 0: 1670, 1: 1670}\n"
     ]
    }
   ],
   "source": [
    "nb_classes = {-1:0,0:0,1:0}\n",
    "y_train_test = keras.utils.to_categorical(y, num_classes=params['n_classes'])\n",
    "for c in y_train_test:\n",
    "    if(np.array_equal([0., 0., 1.],c)):\n",
    "        nb_classes[0]+=1\n",
    "    if(np.array_equal([0., 1., 0.],c)):\n",
    "        nb_classes[-1]+=1\n",
    "    if(np.array_equal([1., 0., 0.],c)):\n",
    "        nb_classes[1]+=1\n",
    "print(nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "  Train: index=[ 334  335  336 ... 5007 5008 5009]\n",
      "  Test:  index=[   0    1    2 ... 3671 3672 3673]\n",
      "{-1: 0, 0: 0, 1: 0}\n",
      "4008 1002\n",
      "Fold 1:\n",
      "  Train: index=[   0    1    2 ... 5007 5008 5009]\n",
      "  Test:  index=[ 334  335  336 ... 4005 4006 4007]\n",
      "{-1: 0, 0: 0, 1: 0}\n",
      "4008 1002\n",
      "Fold 2:\n",
      "  Train: index=[   0    1    2 ... 5007 5008 5009]\n",
      "  Test:  index=[ 668  669  670 ... 4339 4340 4341]\n",
      "{-1: 0, 0: 0, 1: 0}\n",
      "4008 1002\n",
      "Fold 3:\n",
      "  Train: index=[   0    1    2 ... 5007 5008 5009]\n",
      "  Test:  index=[1002 1003 1004 ... 4673 4674 4675]\n",
      "{-1: 0, 0: 0, 1: 0}\n",
      "4008 1002\n",
      "Fold 4:\n",
      "  Train: index=[   0    1    2 ... 4673 4674 4675]\n",
      "  Test:  index=[1336 1337 1338 ... 5007 5008 5009]\n",
      "{-1: 0, 0: 0, 1: 0}\n",
      "4008 1002\n"
     ]
    }
   ],
   "source": [
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}\")\n",
    "    print(f\"  Test:  index={test_index}\")\n",
    "    # Diviser les données d'entraînement et de validation pour ce pli\n",
    "    X_train, X_val = X[train_index], X[test_index]\n",
    "    y_train, y_val = y[train_index], y[test_index]\n",
    "    nb_classes = {-1:0,0:0,1:0}\n",
    "    for c in y_train:\n",
    "        if(np.array_equal([0., 0., 1.],c)):\n",
    "            nb_classes[0]+=1\n",
    "        if(np.array_equal([0., 1., 0.],c)):\n",
    "            nb_classes[-1]+=1\n",
    "        if(np.array_equal([1., 0., 0.],c)):\n",
    "            nb_classes[1]+=1\n",
    "    print(nb_classes)\n",
    "    print(len(X_train),len(X_val))\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes=params['n_classes'])\n",
    "    y_val = keras.utils.to_categorical(y_val, num_classes=params['n_classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Epoch 1/50\n",
      "126/126 [==============================] - 36s 114ms/step - loss: 0.6009 - accuracy: 0.6385 - val_loss: 0.5365 - val_accuracy: 0.8024\n",
      "Epoch 2/50\n",
      "126/126 [==============================] - 11s 85ms/step - loss: 0.5037 - accuracy: 0.7642 - val_loss: 0.4607 - val_accuracy: 0.8623\n",
      "Epoch 3/50\n",
      "126/126 [==============================] - 10s 83ms/step - loss: 0.4438 - accuracy: 0.7762 - val_loss: 0.4079 - val_accuracy: 0.8613\n",
      "Epoch 4/50\n",
      "126/126 [==============================] - 11s 83ms/step - loss: 0.4020 - accuracy: 0.7794 - val_loss: 0.3702 - val_accuracy: 0.8553\n",
      "Epoch 5/50\n",
      "126/126 [==============================] - 10s 83ms/step - loss: 0.3727 - accuracy: 0.7787 - val_loss: 0.3436 - val_accuracy: 0.8653\n",
      "Epoch 6/50\n",
      "126/126 [==============================] - 10s 82ms/step - loss: 0.3513 - accuracy: 0.7817 - val_loss: 0.3228 - val_accuracy: 0.8603\n",
      "Epoch 7/50\n",
      "126/126 [==============================] - 10s 83ms/step - loss: 0.3349 - accuracy: 0.7857 - val_loss: 0.3089 - val_accuracy: 0.8533\n",
      "Epoch 8/50\n",
      "126/126 [==============================] - 10s 83ms/step - loss: 0.3231 - accuracy: 0.7817 - val_loss: 0.2951 - val_accuracy: 0.8603\n",
      "Epoch 9/50\n",
      "126/126 [==============================] - 10s 83ms/step - loss: 0.3131 - accuracy: 0.7822 - val_loss: 0.2841 - val_accuracy: 0.8603\n",
      "Epoch 10/50\n",
      "126/126 [==============================] - 11s 83ms/step - loss: 0.3051 - accuracy: 0.7832 - val_loss: 0.2767 - val_accuracy: 0.8663\n",
      "Epoch 11/50\n",
      "126/126 [==============================] - 10s 83ms/step - loss: 0.2989 - accuracy: 0.7852 - val_loss: 0.2692 - val_accuracy: 0.8623\n",
      "Epoch 12/50\n",
      "126/126 [==============================] - 10s 83ms/step - loss: 0.2937 - accuracy: 0.7847 - val_loss: 0.2613 - val_accuracy: 0.8643\n",
      "Epoch 13/50\n",
      "126/126 [==============================] - 10s 83ms/step - loss: 0.2890 - accuracy: 0.7894 - val_loss: 0.2546 - val_accuracy: 0.8683\n",
      "Epoch 14/50\n",
      "126/126 [==============================] - 10s 83ms/step - loss: 0.2852 - accuracy: 0.7857 - val_loss: 0.2501 - val_accuracy: 0.8713\n",
      "Epoch 15/50\n",
      "126/126 [==============================] - 11s 84ms/step - loss: 0.2816 - accuracy: 0.7877 - val_loss: 0.2527 - val_accuracy: 0.8583\n",
      "Epoch 16/50\n",
      "126/126 [==============================] - 11s 84ms/step - loss: 0.2788 - accuracy: 0.7877 - val_loss: 0.2477 - val_accuracy: 0.8653\n",
      "Epoch 17/50\n",
      "126/126 [==============================] - 10s 83ms/step - loss: 0.2757 - accuracy: 0.7924 - val_loss: 0.2414 - val_accuracy: 0.8683\n",
      "Epoch 18/50\n",
      "126/126 [==============================] - 11s 87ms/step - loss: 0.2738 - accuracy: 0.7869 - val_loss: 0.2395 - val_accuracy: 0.8683\n",
      "Epoch 19/50\n",
      "126/126 [==============================] - 11s 84ms/step - loss: 0.2720 - accuracy: 0.7922 - val_loss: 0.2336 - val_accuracy: 0.8733\n",
      "Epoch 20/50\n",
      "126/126 [==============================] - 11s 85ms/step - loss: 0.2699 - accuracy: 0.7899 - val_loss: 0.2371 - val_accuracy: 0.8653\n",
      "Epoch 21/50\n",
      "126/126 [==============================] - 11s 86ms/step - loss: 0.2687 - accuracy: 0.7964 - val_loss: 0.2303 - val_accuracy: 0.8723\n",
      "Epoch 22/50\n",
      "126/126 [==============================] - 11s 84ms/step - loss: 0.2666 - accuracy: 0.7957 - val_loss: 0.2315 - val_accuracy: 0.8733\n",
      "Epoch 23/50\n",
      "126/126 [==============================] - 11s 84ms/step - loss: 0.2651 - accuracy: 0.7919 - val_loss: 0.2281 - val_accuracy: 0.8713\n",
      "Epoch 24/50\n",
      "126/126 [==============================] - 10s 83ms/step - loss: 0.2642 - accuracy: 0.7949 - val_loss: 0.2237 - val_accuracy: 0.8752\n",
      "Epoch 25/50\n",
      "126/126 [==============================] - 11s 87ms/step - loss: 0.2624 - accuracy: 0.7957 - val_loss: 0.2254 - val_accuracy: 0.8713\n",
      "Epoch 26/50\n",
      "126/126 [==============================] - 11s 84ms/step - loss: 0.2618 - accuracy: 0.7972 - val_loss: 0.2228 - val_accuracy: 0.8743\n",
      "Epoch 27/50\n",
      "126/126 [==============================] - 11s 87ms/step - loss: 0.2599 - accuracy: 0.7989 - val_loss: 0.2276 - val_accuracy: 0.8683\n",
      "Epoch 28/50\n",
      "126/126 [==============================] - 11s 84ms/step - loss: 0.2597 - accuracy: 0.7964 - val_loss: 0.2195 - val_accuracy: 0.8743\n",
      "Epoch 29/50\n",
      "126/126 [==============================] - 11s 84ms/step - loss: 0.2583 - accuracy: 0.7987 - val_loss: 0.2182 - val_accuracy: 0.8743\n",
      "Epoch 30/50\n",
      "126/126 [==============================] - 11s 84ms/step - loss: 0.2584 - accuracy: 0.8009 - val_loss: 0.2167 - val_accuracy: 0.8752\n",
      "Epoch 31/50\n",
      "126/126 [==============================] - 11s 84ms/step - loss: 0.2570 - accuracy: 0.8016 - val_loss: 0.2242 - val_accuracy: 0.8673\n",
      "Epoch 32/50\n",
      "126/126 [==============================] - 11s 84ms/step - loss: 0.2560 - accuracy: 0.8034 - val_loss: 0.2173 - val_accuracy: 0.8752\n",
      "Epoch 33/50\n",
      "126/126 [==============================] - 11s 84ms/step - loss: 0.2553 - accuracy: 0.8006 - val_loss: 0.2115 - val_accuracy: 0.8832\n",
      "Epoch 34/50\n",
      "126/126 [==============================] - 11s 84ms/step - loss: 0.2549 - accuracy: 0.8059 - val_loss: 0.2201 - val_accuracy: 0.8713\n",
      "Epoch 35/50\n",
      "126/126 [==============================] - 11s 84ms/step - loss: 0.2535 - accuracy: 0.8036 - val_loss: 0.2177 - val_accuracy: 0.8733\n",
      "Epoch 36/50\n",
      "126/126 [==============================] - 11s 85ms/step - loss: 0.2536 - accuracy: 0.8029 - val_loss: 0.2187 - val_accuracy: 0.8713\n",
      "Epoch 37/50\n",
      "126/126 [==============================] - 11s 84ms/step - loss: 0.2517 - accuracy: 0.8029 - val_loss: 0.2197 - val_accuracy: 0.8683\n",
      "Epoch 38/50\n",
      "126/126 [==============================] - 11s 84ms/step - loss: 0.2513 - accuracy: 0.8046 - val_loss: 0.2097 - val_accuracy: 0.8842\n",
      "Epoch 39/50\n",
      "126/126 [==============================] - 11s 84ms/step - loss: 0.2508 - accuracy: 0.8031 - val_loss: 0.2181 - val_accuracy: 0.8673\n",
      "Epoch 40/50\n",
      "126/126 [==============================] - 11s 87ms/step - loss: 0.2498 - accuracy: 0.8079 - val_loss: 0.2063 - val_accuracy: 0.8902\n",
      "Epoch 41/50\n",
      "126/126 [==============================] - 11s 84ms/step - loss: 0.2497 - accuracy: 0.8071 - val_loss: 0.2176 - val_accuracy: 0.8643\n",
      "Epoch 42/50\n",
      "126/126 [==============================] - 11s 84ms/step - loss: 0.2490 - accuracy: 0.8054 - val_loss: 0.2059 - val_accuracy: 0.8902\n",
      "Epoch 43/50\n",
      "126/126 [==============================] - 11s 84ms/step - loss: 0.2488 - accuracy: 0.8066 - val_loss: 0.2064 - val_accuracy: 0.8812\n",
      "Epoch 44/50\n",
      "126/126 [==============================] - 11s 84ms/step - loss: 0.2473 - accuracy: 0.8099 - val_loss: 0.2104 - val_accuracy: 0.8723\n",
      "Epoch 45/50\n",
      "126/126 [==============================] - 11s 84ms/step - loss: 0.2470 - accuracy: 0.8091 - val_loss: 0.2044 - val_accuracy: 0.8912\n",
      "Epoch 46/50\n",
      "126/126 [==============================] - 11s 84ms/step - loss: 0.2465 - accuracy: 0.8121 - val_loss: 0.2091 - val_accuracy: 0.8713\n",
      "Epoch 47/50\n",
      "126/126 [==============================] - 11s 84ms/step - loss: 0.2462 - accuracy: 0.8121 - val_loss: 0.2141 - val_accuracy: 0.8643\n",
      "Epoch 48/50\n",
      "126/126 [==============================] - 11s 84ms/step - loss: 0.2457 - accuracy: 0.8124 - val_loss: 0.2162 - val_accuracy: 0.8633\n",
      "Epoch 49/50\n",
      "126/126 [==============================] - 11s 84ms/step - loss: 0.2451 - accuracy: 0.8084 - val_loss: 0.2042 - val_accuracy: 0.8792\n",
      "Epoch 50/50\n",
      "126/126 [==============================] - 11s 87ms/step - loss: 0.2443 - accuracy: 0.8106 - val_loss: 0.2091 - val_accuracy: 0.8683\n",
      "Fold 0 fine tuning\n",
      "Epoch 1/100\n",
      "126/126 [==============================] - 18s 112ms/step - loss: 0.2409 - accuracy: 0.8184 - val_loss: 0.2124 - val_accuracy: 0.8483\n",
      "Epoch 2/100\n",
      "126/126 [==============================] - 13s 105ms/step - loss: 0.2313 - accuracy: 0.8216 - val_loss: 0.1882 - val_accuracy: 0.8912\n",
      "Epoch 3/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.2249 - accuracy: 0.8331 - val_loss: 0.1865 - val_accuracy: 0.8892\n",
      "Epoch 4/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.2203 - accuracy: 0.8383 - val_loss: 0.1905 - val_accuracy: 0.8613\n",
      "Epoch 5/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.2184 - accuracy: 0.8358 - val_loss: 0.1735 - val_accuracy: 0.8932\n",
      "Epoch 6/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.2137 - accuracy: 0.8481 - val_loss: 0.1733 - val_accuracy: 0.8952\n",
      "Epoch 7/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.2085 - accuracy: 0.8528 - val_loss: 0.1720 - val_accuracy: 0.8882\n",
      "Epoch 8/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.2060 - accuracy: 0.8528 - val_loss: 0.1657 - val_accuracy: 0.8942\n",
      "Epoch 9/100\n",
      "126/126 [==============================] - 14s 109ms/step - loss: 0.1999 - accuracy: 0.8570 - val_loss: 0.1689 - val_accuracy: 0.8902\n",
      "Epoch 10/100\n",
      "126/126 [==============================] - 14s 110ms/step - loss: 0.1968 - accuracy: 0.8638 - val_loss: 0.1784 - val_accuracy: 0.8752\n",
      "Epoch 11/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.1916 - accuracy: 0.8675 - val_loss: 0.1601 - val_accuracy: 0.8932\n",
      "Epoch 12/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.1952 - accuracy: 0.8663 - val_loss: 0.1805 - val_accuracy: 0.8703\n",
      "Epoch 13/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.1866 - accuracy: 0.8728 - val_loss: 0.1600 - val_accuracy: 0.8972\n",
      "Epoch 14/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.1818 - accuracy: 0.8772 - val_loss: 0.1790 - val_accuracy: 0.8723\n",
      "Epoch 15/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.1797 - accuracy: 0.8767 - val_loss: 0.1567 - val_accuracy: 0.9102\n",
      "Epoch 16/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.1758 - accuracy: 0.8822 - val_loss: 0.1542 - val_accuracy: 0.8972\n",
      "Epoch 17/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.1755 - accuracy: 0.8822 - val_loss: 0.2018 - val_accuracy: 0.8513\n",
      "Epoch 18/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.1743 - accuracy: 0.8817 - val_loss: 0.1488 - val_accuracy: 0.9052\n",
      "Epoch 19/100\n",
      "126/126 [==============================] - 14s 109ms/step - loss: 0.1725 - accuracy: 0.8842 - val_loss: 0.1503 - val_accuracy: 0.9112\n",
      "Epoch 20/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.1655 - accuracy: 0.8920 - val_loss: 0.1513 - val_accuracy: 0.9152\n",
      "Epoch 21/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.1645 - accuracy: 0.8950 - val_loss: 0.1654 - val_accuracy: 0.8792\n",
      "Epoch 22/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.1652 - accuracy: 0.8905 - val_loss: 0.1413 - val_accuracy: 0.9142\n",
      "Epoch 23/100\n",
      "126/126 [==============================] - 14s 109ms/step - loss: 0.1589 - accuracy: 0.8965 - val_loss: 0.1473 - val_accuracy: 0.9032\n",
      "Epoch 24/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.1552 - accuracy: 0.9034 - val_loss: 0.1465 - val_accuracy: 0.9032\n",
      "Epoch 25/100\n",
      "126/126 [==============================] - 13s 107ms/step - loss: 0.1522 - accuracy: 0.9052 - val_loss: 0.1595 - val_accuracy: 0.8842\n",
      "Epoch 26/100\n",
      "126/126 [==============================] - 13s 107ms/step - loss: 0.1526 - accuracy: 0.9012 - val_loss: 0.1437 - val_accuracy: 0.9062\n",
      "Epoch 27/100\n",
      "126/126 [==============================] - 14s 107ms/step - loss: 0.1510 - accuracy: 0.9034 - val_loss: 0.1637 - val_accuracy: 0.8792\n",
      "Epoch 28/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.1470 - accuracy: 0.9047 - val_loss: 0.1441 - val_accuracy: 0.9022\n",
      "Epoch 29/100\n",
      "126/126 [==============================] - 14s 109ms/step - loss: 0.1447 - accuracy: 0.9077 - val_loss: 0.1422 - val_accuracy: 0.9162\n",
      "Epoch 30/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.1424 - accuracy: 0.9152 - val_loss: 0.1435 - val_accuracy: 0.8992\n",
      "Epoch 31/100\n",
      "126/126 [==============================] - 13s 107ms/step - loss: 0.1404 - accuracy: 0.9114 - val_loss: 0.1416 - val_accuracy: 0.8992\n",
      "Epoch 32/100\n",
      "126/126 [==============================] - 14s 110ms/step - loss: 0.1408 - accuracy: 0.9102 - val_loss: 0.1461 - val_accuracy: 0.9062\n",
      "Epoch 33/100\n",
      "126/126 [==============================] - 14s 107ms/step - loss: 0.1406 - accuracy: 0.9097 - val_loss: 0.1454 - val_accuracy: 0.8922\n",
      "Epoch 34/100\n",
      "126/126 [==============================] - 13s 107ms/step - loss: 0.1377 - accuracy: 0.9159 - val_loss: 0.1440 - val_accuracy: 0.9102\n",
      "Epoch 35/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.1313 - accuracy: 0.9199 - val_loss: 0.1391 - val_accuracy: 0.9072\n",
      "Epoch 36/100\n",
      "126/126 [==============================] - 13s 107ms/step - loss: 0.1294 - accuracy: 0.9229 - val_loss: 0.1510 - val_accuracy: 0.8902\n",
      "Epoch 37/100\n",
      "126/126 [==============================] - 14s 114ms/step - loss: 0.1307 - accuracy: 0.9204 - val_loss: 0.1489 - val_accuracy: 0.8952\n",
      "Epoch 38/100\n",
      "126/126 [==============================] - 14s 112ms/step - loss: 0.1262 - accuracy: 0.9247 - val_loss: 0.1360 - val_accuracy: 0.9062\n",
      "Epoch 39/100\n",
      "126/126 [==============================] - 14s 108ms/step - loss: 0.1234 - accuracy: 0.9247 - val_loss: 0.1380 - val_accuracy: 0.9072\n",
      "Epoch 40/100\n",
      "126/126 [==============================] - 14s 110ms/step - loss: 0.1214 - accuracy: 0.9301 - val_loss: 0.1415 - val_accuracy: 0.8982\n",
      "Epoch 41/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.1211 - accuracy: 0.9239 - val_loss: 0.1435 - val_accuracy: 0.9022\n",
      "Epoch 42/100\n",
      "126/126 [==============================] - 14s 110ms/step - loss: 0.1188 - accuracy: 0.9276 - val_loss: 0.1384 - val_accuracy: 0.9042\n",
      "Epoch 43/100\n",
      "126/126 [==============================] - 14s 110ms/step - loss: 0.1155 - accuracy: 0.9349 - val_loss: 0.1818 - val_accuracy: 0.8653\n",
      "Epoch 44/100\n",
      "126/126 [==============================] - 13s 107ms/step - loss: 0.1211 - accuracy: 0.9249 - val_loss: 0.1367 - val_accuracy: 0.9082\n",
      "Epoch 45/100\n",
      "126/126 [==============================] - 14s 108ms/step - loss: 0.1146 - accuracy: 0.9329 - val_loss: 0.1456 - val_accuracy: 0.8982\n",
      "Epoch 46/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.1106 - accuracy: 0.9359 - val_loss: 0.1340 - val_accuracy: 0.9152\n",
      "Epoch 47/100\n",
      "126/126 [==============================] - 14s 109ms/step - loss: 0.1099 - accuracy: 0.9356 - val_loss: 0.1362 - val_accuracy: 0.9032\n",
      "Epoch 48/100\n",
      "126/126 [==============================] - 14s 108ms/step - loss: 0.1084 - accuracy: 0.9394 - val_loss: 0.1364 - val_accuracy: 0.9082\n",
      "Epoch 49/100\n",
      "126/126 [==============================] - 13s 104ms/step - loss: 0.1070 - accuracy: 0.9411 - val_loss: 0.1373 - val_accuracy: 0.9022\n",
      "Epoch 50/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.1064 - accuracy: 0.9439 - val_loss: 0.1360 - val_accuracy: 0.9022\n",
      "Epoch 51/100\n",
      "126/126 [==============================] - 13s 104ms/step - loss: 0.1083 - accuracy: 0.9366 - val_loss: 0.1415 - val_accuracy: 0.8992\n",
      "Epoch 52/100\n",
      "126/126 [==============================] - 13s 107ms/step - loss: 0.1030 - accuracy: 0.9434 - val_loss: 0.1437 - val_accuracy: 0.8992\n",
      "Epoch 53/100\n",
      "126/126 [==============================] - 14s 108ms/step - loss: 0.0988 - accuracy: 0.9461 - val_loss: 0.1667 - val_accuracy: 0.8782\n",
      "Epoch 54/100\n",
      "126/126 [==============================] - 13s 107ms/step - loss: 0.1027 - accuracy: 0.9416 - val_loss: 0.1421 - val_accuracy: 0.8932\n",
      "Epoch 55/100\n",
      "126/126 [==============================] - 14s 107ms/step - loss: 0.0980 - accuracy: 0.9449 - val_loss: 0.1393 - val_accuracy: 0.9002\n",
      "Epoch 56/100\n",
      "126/126 [==============================] - 13s 105ms/step - loss: 0.1018 - accuracy: 0.9411 - val_loss: 0.1678 - val_accuracy: 0.8782\n",
      "Epoch 57/100\n",
      "126/126 [==============================] - 13s 104ms/step - loss: 0.0970 - accuracy: 0.9454 - val_loss: 0.1598 - val_accuracy: 0.8822\n",
      "Epoch 58/100\n",
      "126/126 [==============================] - 13s 104ms/step - loss: 0.0945 - accuracy: 0.9441 - val_loss: 0.1349 - val_accuracy: 0.9032\n",
      "Epoch 59/100\n",
      "126/126 [==============================] - 13s 104ms/step - loss: 0.0909 - accuracy: 0.9506 - val_loss: 0.1375 - val_accuracy: 0.9012\n",
      "Epoch 60/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.0893 - accuracy: 0.9506 - val_loss: 0.1367 - val_accuracy: 0.9062\n",
      "Epoch 61/100\n",
      "126/126 [==============================] - 13s 104ms/step - loss: 0.0881 - accuracy: 0.9511 - val_loss: 0.1571 - val_accuracy: 0.8862\n",
      "Epoch 62/100\n",
      "126/126 [==============================] - 13s 104ms/step - loss: 0.0934 - accuracy: 0.9456 - val_loss: 0.1380 - val_accuracy: 0.9022\n",
      "Epoch 63/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.0853 - accuracy: 0.9531 - val_loss: 0.1363 - val_accuracy: 0.9032\n",
      "Epoch 64/100\n",
      "126/126 [==============================] - 13s 104ms/step - loss: 0.0837 - accuracy: 0.9576 - val_loss: 0.1357 - val_accuracy: 0.9112\n",
      "Epoch 65/100\n",
      "126/126 [==============================] - 13s 104ms/step - loss: 0.0877 - accuracy: 0.9521 - val_loss: 0.1553 - val_accuracy: 0.8902\n",
      "Epoch 66/100\n",
      "126/126 [==============================] - 13s 104ms/step - loss: 0.0814 - accuracy: 0.9561 - val_loss: 0.1424 - val_accuracy: 0.8972\n",
      "Epoch 67/100\n",
      "126/126 [==============================] - 13s 104ms/step - loss: 0.0851 - accuracy: 0.9553 - val_loss: 0.1386 - val_accuracy: 0.9022\n",
      "Epoch 68/100\n",
      "126/126 [==============================] - 13s 104ms/step - loss: 0.0799 - accuracy: 0.9576 - val_loss: 0.1395 - val_accuracy: 0.8992\n",
      "Epoch 69/100\n",
      "126/126 [==============================] - 13s 103ms/step - loss: 0.0772 - accuracy: 0.9586 - val_loss: 0.1433 - val_accuracy: 0.8912\n",
      "Epoch 70/100\n",
      "126/126 [==============================] - 13s 104ms/step - loss: 0.0807 - accuracy: 0.9573 - val_loss: 0.1380 - val_accuracy: 0.9032\n",
      "Epoch 71/100\n",
      "126/126 [==============================] - 13s 103ms/step - loss: 0.0752 - accuracy: 0.9628 - val_loss: 0.1451 - val_accuracy: 0.8972\n",
      "Epoch 72/100\n",
      "126/126 [==============================] - 13s 103ms/step - loss: 0.0730 - accuracy: 0.9621 - val_loss: 0.1482 - val_accuracy: 0.8942\n",
      "Epoch 73/100\n",
      "126/126 [==============================] - 13s 104ms/step - loss: 0.0733 - accuracy: 0.9633 - val_loss: 0.1459 - val_accuracy: 0.9102\n",
      "Epoch 74/100\n",
      "126/126 [==============================] - 13s 104ms/step - loss: 0.0720 - accuracy: 0.9633 - val_loss: 0.1377 - val_accuracy: 0.9052\n",
      "Epoch 75/100\n",
      "126/126 [==============================] - 13s 104ms/step - loss: 0.0714 - accuracy: 0.9616 - val_loss: 0.1491 - val_accuracy: 0.8912\n",
      "Epoch 76/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.0711 - accuracy: 0.9648 - val_loss: 0.1391 - val_accuracy: 0.9002\n",
      "Epoch 77/100\n",
      "126/126 [==============================] - 13s 104ms/step - loss: 0.0676 - accuracy: 0.9673 - val_loss: 0.1612 - val_accuracy: 0.8882\n",
      "Epoch 78/100\n",
      "126/126 [==============================] - 13s 104ms/step - loss: 0.0652 - accuracy: 0.9688 - val_loss: 0.1536 - val_accuracy: 0.8922\n",
      "Epoch 79/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.0647 - accuracy: 0.9676 - val_loss: 0.1392 - val_accuracy: 0.9002\n",
      "Epoch 80/100\n",
      "126/126 [==============================] - 13s 107ms/step - loss: 0.0700 - accuracy: 0.9663 - val_loss: 0.1559 - val_accuracy: 0.8932\n",
      "Epoch 81/100\n",
      "126/126 [==============================] - 14s 107ms/step - loss: 0.0630 - accuracy: 0.9673 - val_loss: 0.1866 - val_accuracy: 0.8723\n",
      "Epoch 82/100\n",
      "126/126 [==============================] - 14s 108ms/step - loss: 0.0627 - accuracy: 0.9683 - val_loss: 0.1358 - val_accuracy: 0.9062\n",
      "Epoch 83/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.0636 - accuracy: 0.9656 - val_loss: 0.1433 - val_accuracy: 0.9082\n",
      "Epoch 84/100\n",
      "126/126 [==============================] - 13s 105ms/step - loss: 0.0604 - accuracy: 0.9696 - val_loss: 0.1588 - val_accuracy: 0.8912\n",
      "Epoch 85/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.0588 - accuracy: 0.9741 - val_loss: 0.1522 - val_accuracy: 0.9042\n",
      "Epoch 86/100\n",
      "126/126 [==============================] - 13s 107ms/step - loss: 0.0619 - accuracy: 0.9673 - val_loss: 0.1436 - val_accuracy: 0.8982\n",
      "Epoch 87/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.0593 - accuracy: 0.9708 - val_loss: 0.1655 - val_accuracy: 0.8902\n",
      "Epoch 88/100\n",
      "126/126 [==============================] - 13s 104ms/step - loss: 0.0597 - accuracy: 0.9686 - val_loss: 0.1364 - val_accuracy: 0.9102\n",
      "Epoch 89/100\n",
      "126/126 [==============================] - 13s 104ms/step - loss: 0.0542 - accuracy: 0.9758 - val_loss: 0.1461 - val_accuracy: 0.8982\n",
      "Epoch 90/100\n",
      "126/126 [==============================] - 13s 105ms/step - loss: 0.0522 - accuracy: 0.9775 - val_loss: 0.1562 - val_accuracy: 0.9102\n",
      "Epoch 91/100\n",
      "126/126 [==============================] - 14s 108ms/step - loss: 0.0572 - accuracy: 0.9721 - val_loss: 0.1523 - val_accuracy: 0.9002\n",
      "Epoch 92/100\n",
      "126/126 [==============================] - 14s 109ms/step - loss: 0.0519 - accuracy: 0.9773 - val_loss: 0.1548 - val_accuracy: 0.8942\n",
      "Epoch 93/100\n",
      "126/126 [==============================] - 14s 112ms/step - loss: 0.0482 - accuracy: 0.9783 - val_loss: 0.1595 - val_accuracy: 0.8942\n",
      "Epoch 94/100\n",
      "126/126 [==============================] - 14s 113ms/step - loss: 0.0505 - accuracy: 0.9770 - val_loss: 0.2006 - val_accuracy: 0.8762\n",
      "Epoch 95/100\n",
      "126/126 [==============================] - 14s 109ms/step - loss: 0.0489 - accuracy: 0.9778 - val_loss: 0.1558 - val_accuracy: 0.8952\n",
      "Epoch 96/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.0490 - accuracy: 0.9783 - val_loss: 0.1462 - val_accuracy: 0.8982\n",
      "Epoch 97/100\n",
      "126/126 [==============================] - 13s 104ms/step - loss: 0.0483 - accuracy: 0.9780 - val_loss: 0.1485 - val_accuracy: 0.9022\n",
      "Epoch 98/100\n",
      "126/126 [==============================] - 13s 103ms/step - loss: 0.0440 - accuracy: 0.9823 - val_loss: 0.1441 - val_accuracy: 0.9052\n",
      "Epoch 99/100\n",
      "126/126 [==============================] - 14s 108ms/step - loss: 0.0452 - accuracy: 0.9790 - val_loss: 0.1745 - val_accuracy: 0.9002\n",
      "Epoch 100/100\n",
      "126/126 [==============================] - 14s 109ms/step - loss: 0.0442 - accuracy: 0.9808 - val_loss: 0.1471 - val_accuracy: 0.9062\n"
     ]
    }
   ],
   "source": [
    "# Train the model for feature extraction\n",
    "# Effectuer la validation croisée\n",
    "for i, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "    if(i==0):\n",
    "        # Create the VGG16 model for feature extraction\n",
    "        base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, n_channels))\n",
    "\n",
    "        # Freeze the layers of the convolutional base\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        # Create the top layers for feature extraction\n",
    "        model = Sequential()\n",
    "        model.add(base_model)\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        # Compile the model for feature extraction\n",
    "        model.compile(optimizer=Adam(learning_rate=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Diviser les données d'entraînement et de validation pour ce pli\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        y_train = keras.utils.to_categorical(y_train, num_classes=params['n_classes'])\n",
    "        y_val = keras.utils.to_categorical(y_val, num_classes=params['n_classes'])\n",
    "\n",
    "\n",
    "        # Entraîner le classifieur sur les données d'entraînement\n",
    "        print(f'Fold {i}')\n",
    "        scores[i]['history'] = model.fit(x=X_train,y=y_train,validation_data=(X_val,y_val),epochs=50)\n",
    "\n",
    "        # Unfreeze the upper layers of the convolutional base\n",
    "        for layer in model.layers[0].layers[15:]:\n",
    "            layer.trainable = True\n",
    "        \n",
    "        # Compile the model for fine-tuning\n",
    "        model.compile(optimizer=Adam(learning_rate=5e-6), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Entraîner le classifieur sur les données d'entraînement\n",
    "        print(f'Fold {i} fine tuning')\n",
    "        scores[i]['history_fine_tuning'] = model.fit(x=X_train,y=y_train,validation_data=(X_val,y_val),epochs=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.concatenate((history.history['accuracy'],history_fine_tuning.history['accuracy']),axis=0))\n",
    "plt.plot(np.concatenate((history.history['val_accuracy'],history_fine_tuning.history['val_accuracy']),axis=0))\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.concatenate((history.history['loss'],history_fine_tuning.history['loss']),axis=0))\n",
    "plt.plot(np.concatenate((history.history['val_loss'],history_fine_tuning.history['val_loss']),axis=0))\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'history': None, 'history_fine_tuning': None},\n",
       " 1: {'history': None, 'history_fine_tuning': None},\n",
       " 2: {'history': None, 'history_fine_tuning': None},\n",
       " 3: {'history': <keras.callbacks.History at 0x15008932bf0>,\n",
       "  'history_fine_tuning': <keras.callbacks.History at 0x15044bd61d0>},\n",
       " 4: {'history': None, 'history_fine_tuning': None}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('history_fine_tuning_0.npy',scores[0]['history_fine_tuning'].history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=np.load('history_0.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.6076133847236633,\n",
       "  0.5119550228118896,\n",
       "  0.45512035489082336,\n",
       "  0.41340169310569763,\n",
       "  0.3831617534160614,\n",
       "  0.36029306054115295,\n",
       "  0.34224456548690796,\n",
       "  0.32872122526168823,\n",
       "  0.31772056221961975,\n",
       "  0.3082601726055145,\n",
       "  0.30121830105781555,\n",
       "  0.2946246862411499,\n",
       "  0.2893848419189453,\n",
       "  0.28565722703933716,\n",
       "  0.2804762125015259,\n",
       "  0.2774817943572998,\n",
       "  0.27415260672569275,\n",
       "  0.27118176221847534,\n",
       "  0.26922252774238586,\n",
       "  0.2672516703605652,\n",
       "  0.26537400484085083,\n",
       "  0.26316073536872864,\n",
       "  0.2615756094455719,\n",
       "  0.2598535418510437,\n",
       "  0.25900721549987793,\n",
       "  0.25737398862838745,\n",
       "  0.2561050355434418,\n",
       "  0.25433868169784546,\n",
       "  0.2543838620185852,\n",
       "  0.25266116857528687,\n",
       "  0.2518019378185272,\n",
       "  0.25076478719711304,\n",
       "  0.24994918704032898,\n",
       "  0.24908329546451569,\n",
       "  0.24815984070301056,\n",
       "  0.24749702215194702,\n",
       "  0.24651531875133514,\n",
       "  0.24589426815509796,\n",
       "  0.2457185834646225,\n",
       "  0.2443080097436905,\n",
       "  0.24430184066295624,\n",
       "  0.24321138858795166,\n",
       "  0.24224723875522614,\n",
       "  0.24308039247989655,\n",
       "  0.240918830037117,\n",
       "  0.2410796731710434,\n",
       "  0.23989419639110565,\n",
       "  0.23957374691963196,\n",
       "  0.23852457106113434,\n",
       "  0.2377665489912033],\n",
       " 'accuracy': [0.6279940009117126,\n",
       "  0.7722055912017822,\n",
       "  0.7697106003761292,\n",
       "  0.7786926031112671,\n",
       "  0.7994012236595154,\n",
       "  0.8063872456550598,\n",
       "  0.7981536984443665,\n",
       "  0.8066367506980896,\n",
       "  0.8033931851387024,\n",
       "  0.8141217827796936,\n",
       "  0.8068862557411194,\n",
       "  0.8068862557411194,\n",
       "  0.8091317415237427,\n",
       "  0.8073852062225342,\n",
       "  0.813622772693634,\n",
       "  0.8093812465667725,\n",
       "  0.8091317415237427,\n",
       "  0.8146207332611084,\n",
       "  0.8123752474784851,\n",
       "  0.8123752474784851,\n",
       "  0.8158682584762573,\n",
       "  0.8138722777366638,\n",
       "  0.8111277222633362,\n",
       "  0.8168662786483765,\n",
       "  0.8146207332611084,\n",
       "  0.8176147937774658,\n",
       "  0.8183632493019104,\n",
       "  0.8218562602996826,\n",
       "  0.8153692483901978,\n",
       "  0.8231037855148315,\n",
       "  0.8231037855148315,\n",
       "  0.8228542804718018,\n",
       "  0.8238523006439209,\n",
       "  0.8231037855148315,\n",
       "  0.8270958065986633,\n",
       "  0.8246008157730103,\n",
       "  0.8241018056869507,\n",
       "  0.8231037855148315,\n",
       "  0.8241018056869507,\n",
       "  0.8260977864265442,\n",
       "  0.8255987763404846,\n",
       "  0.8253493309020996,\n",
       "  0.8250998258590698,\n",
       "  0.8208582997322083,\n",
       "  0.8303393125534058,\n",
       "  0.8288423418998718,\n",
       "  0.830089807510376,\n",
       "  0.8265967965126038,\n",
       "  0.8313373327255249,\n",
       "  0.8310878276824951],\n",
       " 'val_loss': [0.5440174341201782,\n",
       "  0.47479119896888733,\n",
       "  0.42684242129325867,\n",
       "  0.38955116271972656,\n",
       "  0.3617115318775177,\n",
       "  0.3421071469783783,\n",
       "  0.32689157128334045,\n",
       "  0.31371641159057617,\n",
       "  0.3030789792537689,\n",
       "  0.2953970432281494,\n",
       "  0.2936747670173645,\n",
       "  0.28237563371658325,\n",
       "  0.29109498858451843,\n",
       "  0.2728497385978699,\n",
       "  0.26380136609077454,\n",
       "  0.2638351023197174,\n",
       "  0.25772082805633545,\n",
       "  0.25295332074165344,\n",
       "  0.2599523365497589,\n",
       "  0.2717471122741699,\n",
       "  0.25383061170578003,\n",
       "  0.2635158598423004,\n",
       "  0.2603825330734253,\n",
       "  0.2519243061542511,\n",
       "  0.2515394389629364,\n",
       "  0.24220812320709229,\n",
       "  0.24847380816936493,\n",
       "  0.23439376056194305,\n",
       "  0.2561500370502472,\n",
       "  0.24707110226154327,\n",
       "  0.2514215111732483,\n",
       "  0.24020302295684814,\n",
       "  0.24475421011447906,\n",
       "  0.2405320405960083,\n",
       "  0.24595946073532104,\n",
       "  0.2521668076515198,\n",
       "  0.23696574568748474,\n",
       "  0.25165244936943054,\n",
       "  0.2342626005411148,\n",
       "  0.24679012596607208,\n",
       "  0.24042510986328125,\n",
       "  0.24450957775115967,\n",
       "  0.22870506346225739,\n",
       "  0.24222016334533691,\n",
       "  0.2417372763156891,\n",
       "  0.23292215168476105,\n",
       "  0.23206160962581635,\n",
       "  0.23545831441879272,\n",
       "  0.2525162100791931,\n",
       "  0.26359325647354126],\n",
       " 'val_accuracy': [0.796407163143158,\n",
       "  0.7634730339050293,\n",
       "  0.727544903755188,\n",
       "  0.7594810128211975,\n",
       "  0.7704590559005737,\n",
       "  0.7614770531654358,\n",
       "  0.7614770531654358,\n",
       "  0.7694610953330994,\n",
       "  0.7704590559005737,\n",
       "  0.772455096244812,\n",
       "  0.7614770531654358,\n",
       "  0.7784430980682373,\n",
       "  0.7594810128211975,\n",
       "  0.7834331393241882,\n",
       "  0.7944111824035645,\n",
       "  0.78742516040802,\n",
       "  0.7934131622314453,\n",
       "  0.8093812465667725,\n",
       "  0.7834331393241882,\n",
       "  0.7654690742492676,\n",
       "  0.78742516040802,\n",
       "  0.7764471173286438,\n",
       "  0.7804391384124756,\n",
       "  0.7864271402359009,\n",
       "  0.7864271402359009,\n",
       "  0.802395224571228,\n",
       "  0.7914171814918518,\n",
       "  0.8253493309020996,\n",
       "  0.78143709897995,\n",
       "  0.7914171814918518,\n",
       "  0.7834331393241882,\n",
       "  0.8003991842269897,\n",
       "  0.7944111824035645,\n",
       "  0.8033931851387024,\n",
       "  0.7924151420593262,\n",
       "  0.7844311594963074,\n",
       "  0.8103792667388916,\n",
       "  0.7834331393241882,\n",
       "  0.8063872456550598,\n",
       "  0.7934131622314453,\n",
       "  0.8043912053108215,\n",
       "  0.7954092025756836,\n",
       "  0.8123752474784851,\n",
       "  0.8013972043991089,\n",
       "  0.802395224571228,\n",
       "  0.8083832263946533,\n",
       "  0.8063872456550598,\n",
       "  0.8073852062225342,\n",
       "  0.7864271402359009,\n",
       "  0.7744510769844055]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_master_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
