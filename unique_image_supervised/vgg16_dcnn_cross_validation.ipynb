{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten,Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "# from ROI_extraction import preprocess_image\n",
    "import cv2\n",
    "import os\n",
    "from ROI_extraction import DataGenerator\n",
    "# Set the path to dataset\n",
    "dataset_path = '../images/3regimes'\n",
    "\n",
    "ids = []\n",
    "labels = {}\n",
    "classes = {'excess':1,'normal':0,'insufficient':-1}\n",
    "for class_name in os.listdir(dataset_path):\n",
    "    class_path = os.path.join(dataset_path, class_name)\n",
    "    if os.path.isdir(class_path):\n",
    "        for filename in os.listdir(class_path) :\n",
    "            if filename.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                img_path = os.path.join(class_path, filename) \n",
    "                ids.append(img_path)\n",
    "                labels[img_path]=classes[class_name]\n",
    "\n",
    "# # Shuffle the list of tuples\n",
    "# random.shuffle(ids)\n",
    "\n",
    "# # Define the split ratio (e.g., 80% for training, 20% for validation)\n",
    "# split_ratio = 0.8\n",
    "\n",
    "# # Calculate the index for splitting\n",
    "# split_index = int(len(ids) * split_ratio)\n",
    "\n",
    "# # Split the shuffled IDs and labels into training and validation sets\n",
    "# train_ids = ids[:split_index]\n",
    "# val_ids = ids[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_gamma(image):\n",
    "    # Convert image to float and normalize to range 0-1\n",
    "    image_normalized = image.astype(float) / 255.0\n",
    "\n",
    "    # Calculate mean R intensity\n",
    "    meanRimg = np.mean(image_normalized[:, :, 2])  # Image is in BGR format\n",
    "    \n",
    "    # Calculate G value\n",
    "    G = 0.74 * np.exp(-3.97 * meanRimg)\n",
    "    \n",
    "    # Apply transformation\n",
    "    transformed_image = np.power(image_normalized, 1 / G)\n",
    "    img_float32 = np.float32(transformed_image)\n",
    "    return img_float32\n",
    "\n",
    "def extract_ROI(original_image):\n",
    "    # Convert to grayscale\n",
    "    gray_image = cv2.cvtColor((original_image*255).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # # Apply histogram normalization\n",
    "    # normalized_image = cv2.equalizeHist(gray_image)\n",
    "    \n",
    "    # Apply median filtering\n",
    "    filtered_image = cv2.medianBlur(gray_image, 5)\n",
    "    \n",
    "    # Apply Otsu's thresholding\n",
    "    _, thresholded_image = cv2.threshold(filtered_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    \n",
    "    # Apply morphological operations\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))\n",
    "    closed_image = cv2.morphologyEx(thresholded_image, cv2.MORPH_CLOSE, kernel)\n",
    "    opened_image = cv2.morphologyEx(closed_image, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    # Find contours in the processed image\n",
    "    contours, _ = cv2.findContours(opened_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Find the contour with the largest area\n",
    "    contour = max(contours, key=cv2.contourArea)\n",
    "    \n",
    "    # Calculate the moments of the contour\n",
    "    M = cv2.moments(contour)\n",
    "    \n",
    "    # Calculate the center of the contour\n",
    "    center_x = int(M[\"m10\"] / M[\"m00\"])\n",
    "    center_y = int(M[\"m01\"] / M[\"m00\"])\n",
    "    \n",
    "    # Calculate the coordinates of the square ROI\n",
    "    roi_size = 100\n",
    "    roi_x = center_x - roi_size // 2\n",
    "    roi_y = center_y - roi_size // 2\n",
    "    \n",
    "    return {'contours':contours,'roi_x':roi_x,'roi_y':roi_y,'roi_size':roi_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input image dimensions\n",
    "img_width, img_height = 100, 100\n",
    "n_channels = 3\n",
    "\n",
    "params = {'dim': (img_height,img_width),\n",
    "          'batch_size': 64,\n",
    "          'n_classes': 3,\n",
    "          'n_channels': n_channels,\n",
    "          'shuffle': False}\n",
    "\n",
    "# Set the number of classes\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generation(list_IDs_temp):\n",
    "    X = []\n",
    "    y = np.empty((len(list_IDs_temp)), dtype=int)\n",
    "    for i, ID in enumerate(list_IDs_temp):\n",
    "        image = cv2.imread(ID)\n",
    "        img_gamma_correct = correct_gamma(image)\n",
    "        ROI = extract_ROI(img_gamma_correct)\n",
    "        ROI = image[ROI['roi_y']:ROI['roi_y']+ROI['roi_size'], ROI['roi_x']:ROI['roi_x']+ROI['roi_size']]\n",
    "        # ROI = cv2.cvtColor(ROI, cv2.COLOR_BGR2GRAY)\n",
    "        X.append(ROI)\n",
    "        # Store class\n",
    "        y[i] = labels[ID]\n",
    "    X = np.reshape(X,(len(list_IDs_temp),img_width, img_height,params['n_channels']))\n",
    "    X = X.astype(\"float32\") / 255.0\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = data_generation(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Nombre de plis pour la validation croisée k-fold\n",
    "k = 5\n",
    "\n",
    "# Créer une instance de StratifiedKFold avec k plis\n",
    "skf = StratifiedKFold(n_splits=k)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste pour stocker les scores de validation\n",
    "scores = {i:{'history':None,'history_fine_tuning':None} for i in range(k)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'history': None, 'history_fine_tuning': None},\n",
       " 1: {'history': None, 'history_fine_tuning': None},\n",
       " 2: {'history': None, 'history_fine_tuning': None},\n",
       " 3: {'history': None, 'history_fine_tuning': None},\n",
       " 4: {'history': None, 'history_fine_tuning': None}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{-1: 1670, 0: 1670, 1: 1670}\n"
     ]
    }
   ],
   "source": [
    "nb_classes = {-1:0,0:0,1:0}\n",
    "y_train_test = keras.utils.to_categorical(y, num_classes=params['n_classes'])\n",
    "for c in y_train_test:\n",
    "    if(np.array_equal([0., 0., 1.],c)):\n",
    "        nb_classes[0]+=1\n",
    "    if(np.array_equal([0., 1., 0.],c)):\n",
    "        nb_classes[-1]+=1\n",
    "    if(np.array_equal([1., 0., 0.],c)):\n",
    "        nb_classes[1]+=1\n",
    "print(nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "  Train: index=[ 334  335  336 ... 5007 5008 5009]\n",
      "  Test:  index=[   0    1    2 ... 3671 3672 3673]\n",
      "{-1: 1336, 0: 1336, 1: 1336}\n",
      "4008 1002\n",
      "Fold 1:\n",
      "  Train: index=[   0    1    2 ... 5007 5008 5009]\n",
      "  Test:  index=[ 334  335  336 ... 4005 4006 4007]\n",
      "{-1: 1336, 0: 1336, 1: 1336}\n",
      "4008 1002\n",
      "Fold 2:\n",
      "  Train: index=[   0    1    2 ... 5007 5008 5009]\n",
      "  Test:  index=[ 668  669  670 ... 4339 4340 4341]\n",
      "{-1: 1336, 0: 1336, 1: 1336}\n",
      "4008 1002\n",
      "Fold 3:\n",
      "  Train: index=[   0    1    2 ... 5007 5008 5009]\n",
      "  Test:  index=[1002 1003 1004 ... 4673 4674 4675]\n",
      "{-1: 1336, 0: 1336, 1: 1336}\n",
      "4008 1002\n",
      "Fold 4:\n",
      "  Train: index=[   0    1    2 ... 4673 4674 4675]\n",
      "  Test:  index=[1336 1337 1338 ... 5007 5008 5009]\n",
      "{-1: 1336, 0: 1336, 1: 1336}\n",
      "4008 1002\n"
     ]
    }
   ],
   "source": [
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    print(f\"Fold {i}:\")\n",
    "    print(f\"  Train: index={train_index}\")\n",
    "    print(f\"  Test:  index={test_index}\")\n",
    "    # Diviser les données d'entraînement et de validation pour ce pli\n",
    "    X_train, X_val = X[train_index], X[test_index]\n",
    "    y_train, y_val = y[train_index], y[test_index]\n",
    "    nb_classes = {-1:0,0:0,1:0}\n",
    "    for c in y_train:\n",
    "        if(np.array_equal([0., 0., 1.],c)):\n",
    "            nb_classes[0]+=1\n",
    "        if(np.array_equal([0., 1., 0.],c)):\n",
    "            nb_classes[-1]+=1\n",
    "        if(np.array_equal([1., 0., 0.],c)):\n",
    "            nb_classes[1]+=1\n",
    "    print(nb_classes)\n",
    "    print(len(X_train),len(X_val))\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes=params['n_classes'])\n",
    "    y_val = keras.utils.to_categorical(y_val, num_classes=params['n_classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4\n",
      "Epoch 1/50\n",
      "126/126 [==============================] - 12s 85ms/step - loss: 0.6015 - accuracy: 0.6407 - val_loss: 0.5589 - val_accuracy: 0.7146\n",
      "Epoch 2/50\n",
      "126/126 [==============================] - 11s 84ms/step - loss: 0.5141 - accuracy: 0.7859 - val_loss: 0.4933 - val_accuracy: 0.7525\n",
      "Epoch 3/50\n",
      "126/126 [==============================] - 11s 87ms/step - loss: 0.4535 - accuracy: 0.7962 - val_loss: 0.4459 - val_accuracy: 0.7505\n",
      "Epoch 4/50\n",
      "126/126 [==============================] - 11s 88ms/step - loss: 0.4078 - accuracy: 0.8074 - val_loss: 0.4145 - val_accuracy: 0.7485\n",
      "Epoch 5/50\n",
      "126/126 [==============================] - 11s 88ms/step - loss: 0.3748 - accuracy: 0.8036 - val_loss: 0.3856 - val_accuracy: 0.7575\n",
      "Epoch 6/50\n",
      "126/126 [==============================] - 11s 87ms/step - loss: 0.3491 - accuracy: 0.8099 - val_loss: 0.3664 - val_accuracy: 0.7246\n",
      "Epoch 7/50\n",
      "126/126 [==============================] - 11s 87ms/step - loss: 0.3299 - accuracy: 0.8081 - val_loss: 0.3528 - val_accuracy: 0.7315\n",
      "Epoch 8/50\n",
      "126/126 [==============================] - 11s 87ms/step - loss: 0.3159 - accuracy: 0.8081 - val_loss: 0.3422 - val_accuracy: 0.7305\n",
      "Epoch 9/50\n",
      "126/126 [==============================] - 11s 89ms/step - loss: 0.3038 - accuracy: 0.8106 - val_loss: 0.3336 - val_accuracy: 0.7315\n",
      "Epoch 10/50\n",
      "126/126 [==============================] - 11s 86ms/step - loss: 0.2946 - accuracy: 0.8126 - val_loss: 0.3272 - val_accuracy: 0.7565\n",
      "Epoch 11/50\n",
      "126/126 [==============================] - 12s 93ms/step - loss: 0.2870 - accuracy: 0.8134 - val_loss: 0.3220 - val_accuracy: 0.7575\n",
      "Epoch 12/50\n",
      "126/126 [==============================] - 12s 92ms/step - loss: 0.2813 - accuracy: 0.8119 - val_loss: 0.3162 - val_accuracy: 0.7365\n",
      "Epoch 13/50\n",
      "126/126 [==============================] - 12s 92ms/step - loss: 0.2753 - accuracy: 0.8134 - val_loss: 0.3120 - val_accuracy: 0.7395\n",
      "Epoch 14/50\n",
      "126/126 [==============================] - 11s 86ms/step - loss: 0.2705 - accuracy: 0.8176 - val_loss: 0.3090 - val_accuracy: 0.7435\n",
      "Epoch 15/50\n",
      "126/126 [==============================] - 11s 85ms/step - loss: 0.2674 - accuracy: 0.8106 - val_loss: 0.3062 - val_accuracy: 0.7415\n",
      "Epoch 16/50\n",
      "126/126 [==============================] - 11s 86ms/step - loss: 0.2643 - accuracy: 0.8141 - val_loss: 0.3055 - val_accuracy: 0.7545\n",
      "Epoch 17/50\n",
      "126/126 [==============================] - 11s 86ms/step - loss: 0.2605 - accuracy: 0.8164 - val_loss: 0.3030 - val_accuracy: 0.7565\n",
      "Epoch 18/50\n",
      "126/126 [==============================] - 11s 85ms/step - loss: 0.2578 - accuracy: 0.8179 - val_loss: 0.3043 - val_accuracy: 0.7595\n",
      "Epoch 19/50\n",
      "126/126 [==============================] - 11s 85ms/step - loss: 0.2559 - accuracy: 0.8211 - val_loss: 0.3044 - val_accuracy: 0.7605\n",
      "Epoch 20/50\n",
      "126/126 [==============================] - 11s 86ms/step - loss: 0.2536 - accuracy: 0.8191 - val_loss: 0.2981 - val_accuracy: 0.7595\n",
      "Epoch 21/50\n",
      "126/126 [==============================] - 11s 85ms/step - loss: 0.2520 - accuracy: 0.8253 - val_loss: 0.2958 - val_accuracy: 0.7505\n",
      "Epoch 22/50\n",
      "126/126 [==============================] - 11s 85ms/step - loss: 0.2498 - accuracy: 0.8216 - val_loss: 0.3009 - val_accuracy: 0.7615\n",
      "Epoch 23/50\n",
      "126/126 [==============================] - 11s 86ms/step - loss: 0.2487 - accuracy: 0.8204 - val_loss: 0.2934 - val_accuracy: 0.7465\n",
      "Epoch 24/50\n",
      "126/126 [==============================] - 11s 86ms/step - loss: 0.2475 - accuracy: 0.8211 - val_loss: 0.2933 - val_accuracy: 0.7555\n",
      "Epoch 25/50\n",
      "126/126 [==============================] - 11s 85ms/step - loss: 0.2456 - accuracy: 0.8216 - val_loss: 0.2934 - val_accuracy: 0.7615\n",
      "Epoch 26/50\n",
      "126/126 [==============================] - 11s 88ms/step - loss: 0.2449 - accuracy: 0.8231 - val_loss: 0.2927 - val_accuracy: 0.7615\n",
      "Epoch 27/50\n",
      "126/126 [==============================] - 11s 85ms/step - loss: 0.2433 - accuracy: 0.8231 - val_loss: 0.2952 - val_accuracy: 0.7675\n",
      "Epoch 28/50\n",
      "126/126 [==============================] - 11s 88ms/step - loss: 0.2420 - accuracy: 0.8244 - val_loss: 0.2904 - val_accuracy: 0.7675\n",
      "Epoch 29/50\n",
      "126/126 [==============================] - 11s 86ms/step - loss: 0.2414 - accuracy: 0.8231 - val_loss: 0.2893 - val_accuracy: 0.7605\n",
      "Epoch 30/50\n",
      "126/126 [==============================] - 11s 86ms/step - loss: 0.2402 - accuracy: 0.8239 - val_loss: 0.2901 - val_accuracy: 0.7695\n",
      "Epoch 31/50\n",
      "126/126 [==============================] - 11s 85ms/step - loss: 0.2395 - accuracy: 0.8244 - val_loss: 0.2877 - val_accuracy: 0.7485\n",
      "Epoch 32/50\n",
      "126/126 [==============================] - 11s 86ms/step - loss: 0.2387 - accuracy: 0.8261 - val_loss: 0.2868 - val_accuracy: 0.7635\n",
      "Epoch 33/50\n",
      "126/126 [==============================] - 11s 85ms/step - loss: 0.2375 - accuracy: 0.8278 - val_loss: 0.2904 - val_accuracy: 0.7685\n",
      "Epoch 34/50\n",
      "126/126 [==============================] - 11s 86ms/step - loss: 0.2371 - accuracy: 0.8273 - val_loss: 0.2897 - val_accuracy: 0.7695\n",
      "Epoch 35/50\n",
      "126/126 [==============================] - 11s 86ms/step - loss: 0.2368 - accuracy: 0.8261 - val_loss: 0.2904 - val_accuracy: 0.7705\n",
      "Epoch 36/50\n",
      "126/126 [==============================] - 11s 86ms/step - loss: 0.2359 - accuracy: 0.8293 - val_loss: 0.2863 - val_accuracy: 0.7695\n",
      "Epoch 37/50\n",
      "126/126 [==============================] - 11s 85ms/step - loss: 0.2354 - accuracy: 0.8288 - val_loss: 0.2856 - val_accuracy: 0.7705\n",
      "Epoch 38/50\n",
      "126/126 [==============================] - 11s 85ms/step - loss: 0.2354 - accuracy: 0.8261 - val_loss: 0.2859 - val_accuracy: 0.7754\n",
      "Epoch 39/50\n",
      "126/126 [==============================] - 11s 85ms/step - loss: 0.2334 - accuracy: 0.8278 - val_loss: 0.2878 - val_accuracy: 0.7764\n",
      "Epoch 40/50\n",
      "126/126 [==============================] - 11s 88ms/step - loss: 0.2328 - accuracy: 0.8263 - val_loss: 0.2830 - val_accuracy: 0.7645\n",
      "Epoch 41/50\n",
      "126/126 [==============================] - 11s 86ms/step - loss: 0.2328 - accuracy: 0.8301 - val_loss: 0.2824 - val_accuracy: 0.7585\n",
      "Epoch 42/50\n",
      "126/126 [==============================] - 11s 89ms/step - loss: 0.2316 - accuracy: 0.8296 - val_loss: 0.2835 - val_accuracy: 0.7764\n",
      "Epoch 43/50\n",
      "126/126 [==============================] - 11s 88ms/step - loss: 0.2307 - accuracy: 0.8331 - val_loss: 0.2838 - val_accuracy: 0.7754\n",
      "Epoch 44/50\n",
      "126/126 [==============================] - 11s 85ms/step - loss: 0.2300 - accuracy: 0.8353 - val_loss: 0.2891 - val_accuracy: 0.7764\n",
      "Epoch 45/50\n",
      "126/126 [==============================] - 11s 85ms/step - loss: 0.2303 - accuracy: 0.8288 - val_loss: 0.2810 - val_accuracy: 0.7715\n",
      "Epoch 46/50\n",
      "126/126 [==============================] - 11s 88ms/step - loss: 0.2289 - accuracy: 0.8316 - val_loss: 0.2863 - val_accuracy: 0.7814\n",
      "Epoch 47/50\n",
      "126/126 [==============================] - 11s 85ms/step - loss: 0.2287 - accuracy: 0.8318 - val_loss: 0.2812 - val_accuracy: 0.7774\n",
      "Epoch 48/50\n",
      "126/126 [==============================] - 11s 86ms/step - loss: 0.2280 - accuracy: 0.8348 - val_loss: 0.2807 - val_accuracy: 0.7745\n",
      "Epoch 49/50\n",
      "126/126 [==============================] - 11s 89ms/step - loss: 0.2277 - accuracy: 0.8348 - val_loss: 0.2801 - val_accuracy: 0.7695\n",
      "Epoch 50/50\n",
      "126/126 [==============================] - 11s 88ms/step - loss: 0.2272 - accuracy: 0.8341 - val_loss: 0.2791 - val_accuracy: 0.7675\n",
      "Fold 4 fine tuning\n",
      "Epoch 1/100\n",
      "126/126 [==============================] - 18s 113ms/step - loss: 0.2225 - accuracy: 0.8366 - val_loss: 0.2773 - val_accuracy: 0.7964\n",
      "Epoch 2/100\n",
      "126/126 [==============================] - 14s 112ms/step - loss: 0.2134 - accuracy: 0.8403 - val_loss: 0.2820 - val_accuracy: 0.7934\n",
      "Epoch 3/100\n",
      "126/126 [==============================] - 15s 123ms/step - loss: 0.2060 - accuracy: 0.8481 - val_loss: 0.2831 - val_accuracy: 0.7904\n",
      "Epoch 4/100\n",
      "126/126 [==============================] - 14s 109ms/step - loss: 0.2009 - accuracy: 0.8535 - val_loss: 0.2661 - val_accuracy: 0.7914\n",
      "Epoch 5/100\n",
      "126/126 [==============================] - 14s 108ms/step - loss: 0.1942 - accuracy: 0.8618 - val_loss: 0.2714 - val_accuracy: 0.8054\n",
      "Epoch 6/100\n",
      "126/126 [==============================] - 14s 108ms/step - loss: 0.1911 - accuracy: 0.8658 - val_loss: 0.2655 - val_accuracy: 0.7854\n",
      "Epoch 7/100\n",
      "126/126 [==============================] - 14s 108ms/step - loss: 0.1866 - accuracy: 0.8693 - val_loss: 0.2633 - val_accuracy: 0.7904\n",
      "Epoch 8/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.1832 - accuracy: 0.8683 - val_loss: 0.2664 - val_accuracy: 0.8044\n",
      "Epoch 9/100\n",
      "126/126 [==============================] - 14s 110ms/step - loss: 0.1818 - accuracy: 0.8708 - val_loss: 0.2645 - val_accuracy: 0.7924\n",
      "Epoch 10/100\n",
      "126/126 [==============================] - 14s 109ms/step - loss: 0.1805 - accuracy: 0.8775 - val_loss: 0.2696 - val_accuracy: 0.8164\n",
      "Epoch 11/100\n",
      "126/126 [==============================] - 15s 118ms/step - loss: 0.1732 - accuracy: 0.8805 - val_loss: 0.2578 - val_accuracy: 0.8054\n",
      "Epoch 12/100\n",
      "126/126 [==============================] - 14s 109ms/step - loss: 0.1697 - accuracy: 0.8837 - val_loss: 0.2662 - val_accuracy: 0.8194\n",
      "Epoch 13/100\n",
      "126/126 [==============================] - 14s 108ms/step - loss: 0.1670 - accuracy: 0.8892 - val_loss: 0.2523 - val_accuracy: 0.8024\n",
      "Epoch 14/100\n",
      "126/126 [==============================] - 14s 109ms/step - loss: 0.1649 - accuracy: 0.8902 - val_loss: 0.2523 - val_accuracy: 0.8154\n",
      "Epoch 15/100\n",
      "126/126 [==============================] - 14s 109ms/step - loss: 0.1644 - accuracy: 0.8885 - val_loss: 0.2567 - val_accuracy: 0.8024\n",
      "Epoch 16/100\n",
      "126/126 [==============================] - 14s 109ms/step - loss: 0.1622 - accuracy: 0.8897 - val_loss: 0.2736 - val_accuracy: 0.8204\n",
      "Epoch 17/100\n",
      "126/126 [==============================] - 14s 108ms/step - loss: 0.1599 - accuracy: 0.8965 - val_loss: 0.2479 - val_accuracy: 0.8124\n",
      "Epoch 18/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.1533 - accuracy: 0.8972 - val_loss: 0.2578 - val_accuracy: 0.8224\n",
      "Epoch 19/100\n",
      "126/126 [==============================] - 14s 108ms/step - loss: 0.1562 - accuracy: 0.8940 - val_loss: 0.2529 - val_accuracy: 0.8024\n",
      "Epoch 20/100\n",
      "126/126 [==============================] - 14s 108ms/step - loss: 0.1501 - accuracy: 0.9000 - val_loss: 0.2609 - val_accuracy: 0.7924\n",
      "Epoch 21/100\n",
      "126/126 [==============================] - 14s 107ms/step - loss: 0.1502 - accuracy: 0.9017 - val_loss: 0.2825 - val_accuracy: 0.8214\n",
      "Epoch 22/100\n",
      "126/126 [==============================] - 13s 107ms/step - loss: 0.1492 - accuracy: 0.9034 - val_loss: 0.2438 - val_accuracy: 0.8244\n",
      "Epoch 23/100\n",
      "126/126 [==============================] - 13s 107ms/step - loss: 0.1468 - accuracy: 0.9027 - val_loss: 0.2518 - val_accuracy: 0.7994\n",
      "Epoch 24/100\n",
      "126/126 [==============================] - 13s 107ms/step - loss: 0.1419 - accuracy: 0.9079 - val_loss: 0.2413 - val_accuracy: 0.8303\n",
      "Epoch 25/100\n",
      "126/126 [==============================] - 14s 108ms/step - loss: 0.1414 - accuracy: 0.9072 - val_loss: 0.2447 - val_accuracy: 0.8224\n",
      "Epoch 26/100\n",
      "126/126 [==============================] - 14s 110ms/step - loss: 0.1384 - accuracy: 0.9119 - val_loss: 0.2429 - val_accuracy: 0.8104\n",
      "Epoch 27/100\n",
      "126/126 [==============================] - 14s 110ms/step - loss: 0.1381 - accuracy: 0.9129 - val_loss: 0.2380 - val_accuracy: 0.8244\n",
      "Epoch 28/100\n",
      "126/126 [==============================] - 13s 107ms/step - loss: 0.1333 - accuracy: 0.9134 - val_loss: 0.2433 - val_accuracy: 0.8303\n",
      "Epoch 29/100\n",
      "126/126 [==============================] - 13s 107ms/step - loss: 0.1340 - accuracy: 0.9152 - val_loss: 0.2428 - val_accuracy: 0.8114\n",
      "Epoch 30/100\n",
      "126/126 [==============================] - 13s 107ms/step - loss: 0.1299 - accuracy: 0.9194 - val_loss: 0.2419 - val_accuracy: 0.8204\n",
      "Epoch 31/100\n",
      "126/126 [==============================] - 14s 110ms/step - loss: 0.1293 - accuracy: 0.9232 - val_loss: 0.2390 - val_accuracy: 0.8224\n",
      "Epoch 32/100\n",
      "126/126 [==============================] - 13s 106ms/step - loss: 0.1290 - accuracy: 0.9194 - val_loss: 0.2607 - val_accuracy: 0.8234\n",
      "Epoch 33/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.1255 - accuracy: 0.9214 - val_loss: 0.2387 - val_accuracy: 0.8244\n",
      "Epoch 34/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.1202 - accuracy: 0.9269 - val_loss: 0.2379 - val_accuracy: 0.8323\n",
      "Epoch 35/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.1200 - accuracy: 0.9284 - val_loss: 0.2400 - val_accuracy: 0.8313\n",
      "Epoch 36/100\n",
      "126/126 [==============================] - 13s 103ms/step - loss: 0.1189 - accuracy: 0.9251 - val_loss: 0.2333 - val_accuracy: 0.8323\n",
      "Epoch 37/100\n",
      "126/126 [==============================] - 13s 103ms/step - loss: 0.1200 - accuracy: 0.9266 - val_loss: 0.2493 - val_accuracy: 0.8343\n",
      "Epoch 38/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.1243 - accuracy: 0.9187 - val_loss: 0.2327 - val_accuracy: 0.8323\n",
      "Epoch 39/100\n",
      "126/126 [==============================] - 13s 103ms/step - loss: 0.1181 - accuracy: 0.9266 - val_loss: 0.2865 - val_accuracy: 0.7984\n",
      "Epoch 40/100\n",
      "126/126 [==============================] - 13s 103ms/step - loss: 0.1174 - accuracy: 0.9281 - val_loss: 0.2331 - val_accuracy: 0.8393\n",
      "Epoch 41/100\n",
      "126/126 [==============================] - 13s 105ms/step - loss: 0.1094 - accuracy: 0.9361 - val_loss: 0.2414 - val_accuracy: 0.8353\n",
      "Epoch 42/100\n",
      "126/126 [==============================] - 13s 104ms/step - loss: 0.1152 - accuracy: 0.9281 - val_loss: 0.2460 - val_accuracy: 0.8353\n",
      "Epoch 43/100\n",
      "126/126 [==============================] - 13s 104ms/step - loss: 0.1082 - accuracy: 0.9329 - val_loss: 0.2321 - val_accuracy: 0.8353\n",
      "Epoch 44/100\n",
      "126/126 [==============================] - 13s 103ms/step - loss: 0.1079 - accuracy: 0.9346 - val_loss: 0.2334 - val_accuracy: 0.8323\n",
      "Epoch 45/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.1049 - accuracy: 0.9381 - val_loss: 0.2318 - val_accuracy: 0.8353\n",
      "Epoch 46/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.1032 - accuracy: 0.9411 - val_loss: 0.2358 - val_accuracy: 0.8393\n",
      "Epoch 47/100\n",
      "126/126 [==============================] - 13s 103ms/step - loss: 0.1023 - accuracy: 0.9401 - val_loss: 0.2400 - val_accuracy: 0.8234\n",
      "Epoch 48/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0993 - accuracy: 0.9441 - val_loss: 0.2337 - val_accuracy: 0.8393\n",
      "Epoch 49/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.1019 - accuracy: 0.9391 - val_loss: 0.2377 - val_accuracy: 0.8413\n",
      "Epoch 50/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0976 - accuracy: 0.9449 - val_loss: 0.2370 - val_accuracy: 0.8363\n",
      "Epoch 51/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0990 - accuracy: 0.9431 - val_loss: 0.2407 - val_accuracy: 0.8403\n",
      "Epoch 52/100\n",
      "126/126 [==============================] - 13s 105ms/step - loss: 0.0969 - accuracy: 0.9406 - val_loss: 0.2267 - val_accuracy: 0.8453\n",
      "Epoch 53/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0913 - accuracy: 0.9481 - val_loss: 0.2431 - val_accuracy: 0.8263\n",
      "Epoch 54/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0918 - accuracy: 0.9506 - val_loss: 0.2272 - val_accuracy: 0.8503\n",
      "Epoch 55/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0912 - accuracy: 0.9508 - val_loss: 0.2527 - val_accuracy: 0.8273\n",
      "Epoch 56/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0893 - accuracy: 0.9484 - val_loss: 0.2329 - val_accuracy: 0.8443\n",
      "Epoch 57/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0974 - accuracy: 0.9426 - val_loss: 0.2368 - val_accuracy: 0.8313\n",
      "Epoch 58/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0853 - accuracy: 0.9528 - val_loss: 0.2238 - val_accuracy: 0.8483\n",
      "Epoch 59/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0850 - accuracy: 0.9548 - val_loss: 0.2338 - val_accuracy: 0.8463\n",
      "Epoch 60/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0871 - accuracy: 0.9508 - val_loss: 0.2354 - val_accuracy: 0.8383\n",
      "Epoch 61/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0845 - accuracy: 0.9521 - val_loss: 0.2256 - val_accuracy: 0.8443\n",
      "Epoch 62/100\n",
      "126/126 [==============================] - 13s 105ms/step - loss: 0.0846 - accuracy: 0.9543 - val_loss: 0.2733 - val_accuracy: 0.8204\n",
      "Epoch 63/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0805 - accuracy: 0.9581 - val_loss: 0.2355 - val_accuracy: 0.8403\n",
      "Epoch 64/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0780 - accuracy: 0.9583 - val_loss: 0.2405 - val_accuracy: 0.8493\n",
      "Epoch 65/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0767 - accuracy: 0.9593 - val_loss: 0.2467 - val_accuracy: 0.8313\n",
      "Epoch 66/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0742 - accuracy: 0.9623 - val_loss: 0.2284 - val_accuracy: 0.8563\n",
      "Epoch 67/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0792 - accuracy: 0.9566 - val_loss: 0.2496 - val_accuracy: 0.8303\n",
      "Epoch 68/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0783 - accuracy: 0.9573 - val_loss: 0.2335 - val_accuracy: 0.8463\n",
      "Epoch 69/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0709 - accuracy: 0.9658 - val_loss: 0.2688 - val_accuracy: 0.8433\n",
      "Epoch 70/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0748 - accuracy: 0.9596 - val_loss: 0.2308 - val_accuracy: 0.8503\n",
      "Epoch 71/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0759 - accuracy: 0.9583 - val_loss: 0.2354 - val_accuracy: 0.8443\n",
      "Epoch 72/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0675 - accuracy: 0.9686 - val_loss: 0.2437 - val_accuracy: 0.8523\n",
      "Epoch 73/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0680 - accuracy: 0.9651 - val_loss: 0.2438 - val_accuracy: 0.8503\n",
      "Epoch 74/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0680 - accuracy: 0.9661 - val_loss: 0.2386 - val_accuracy: 0.8543\n",
      "Epoch 75/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0652 - accuracy: 0.9698 - val_loss: 0.2202 - val_accuracy: 0.8583\n",
      "Epoch 76/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0649 - accuracy: 0.9678 - val_loss: 0.2444 - val_accuracy: 0.8453\n",
      "Epoch 77/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0659 - accuracy: 0.9671 - val_loss: 0.2556 - val_accuracy: 0.8373\n",
      "Epoch 78/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0671 - accuracy: 0.9668 - val_loss: 0.2536 - val_accuracy: 0.8393\n",
      "Epoch 79/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0623 - accuracy: 0.9678 - val_loss: 0.2345 - val_accuracy: 0.8543\n",
      "Epoch 80/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0598 - accuracy: 0.9721 - val_loss: 0.2402 - val_accuracy: 0.8553\n",
      "Epoch 81/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0612 - accuracy: 0.9703 - val_loss: 0.2633 - val_accuracy: 0.8433\n",
      "Epoch 82/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0581 - accuracy: 0.9713 - val_loss: 0.2284 - val_accuracy: 0.8553\n",
      "Epoch 83/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0617 - accuracy: 0.9691 - val_loss: 0.2331 - val_accuracy: 0.8563\n",
      "Epoch 84/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0557 - accuracy: 0.9736 - val_loss: 0.2421 - val_accuracy: 0.8513\n",
      "Epoch 85/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0562 - accuracy: 0.9731 - val_loss: 0.2563 - val_accuracy: 0.8383\n",
      "Epoch 86/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0558 - accuracy: 0.9716 - val_loss: 0.2356 - val_accuracy: 0.8583\n",
      "Epoch 87/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0519 - accuracy: 0.9780 - val_loss: 0.2485 - val_accuracy: 0.8563\n",
      "Epoch 88/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0526 - accuracy: 0.9741 - val_loss: 0.2547 - val_accuracy: 0.8463\n",
      "Epoch 89/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0520 - accuracy: 0.9746 - val_loss: 0.2559 - val_accuracy: 0.8443\n",
      "Epoch 90/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0498 - accuracy: 0.9773 - val_loss: 0.2806 - val_accuracy: 0.8403\n",
      "Epoch 91/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0494 - accuracy: 0.9746 - val_loss: 0.2506 - val_accuracy: 0.8493\n",
      "Epoch 92/100\n",
      "126/126 [==============================] - 13s 105ms/step - loss: 0.0489 - accuracy: 0.9768 - val_loss: 0.2665 - val_accuracy: 0.8463\n",
      "Epoch 93/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0457 - accuracy: 0.9798 - val_loss: 0.2639 - val_accuracy: 0.8453\n",
      "Epoch 94/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0494 - accuracy: 0.9753 - val_loss: 0.2577 - val_accuracy: 0.8483\n",
      "Epoch 95/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0506 - accuracy: 0.9743 - val_loss: 0.2389 - val_accuracy: 0.8653\n",
      "Epoch 96/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0498 - accuracy: 0.9750 - val_loss: 0.2482 - val_accuracy: 0.8633\n",
      "Epoch 97/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0506 - accuracy: 0.9733 - val_loss: 0.2666 - val_accuracy: 0.8453\n",
      "Epoch 98/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0402 - accuracy: 0.9835 - val_loss: 0.2618 - val_accuracy: 0.8573\n",
      "Epoch 99/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0415 - accuracy: 0.9840 - val_loss: 0.2623 - val_accuracy: 0.8453\n",
      "Epoch 100/100\n",
      "126/126 [==============================] - 13s 102ms/step - loss: 0.0449 - accuracy: 0.9800 - val_loss: 0.2535 - val_accuracy: 0.8633\n"
     ]
    }
   ],
   "source": [
    "# Train the model for feature extraction\n",
    "# Effectuer la validation croisée\n",
    "for i, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "    if(i==0):\n",
    "        # Create the VGG16 model for feature extraction\n",
    "        base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, n_channels))\n",
    "\n",
    "        # Freeze the layers of the convolutional base\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        # Create the top layers for feature extraction\n",
    "        model = Sequential()\n",
    "        model.add(base_model)\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        # Compile the model for feature extraction\n",
    "        model.compile(optimizer=Adam(learning_rate=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Diviser les données d'entraînement et de validation pour ce pli\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        y_train = keras.utils.to_categorical(y_train, num_classes=params['n_classes'])\n",
    "        y_val = keras.utils.to_categorical(y_val, num_classes=params['n_classes'])\n",
    "\n",
    "\n",
    "        # Entraîner le classifieur sur les données d'entraînement\n",
    "        print(f'Fold {i}')\n",
    "        scores[i]['history'] = model.fit(x=X_train,y=y_train,validation_data=(X_val,y_val),epochs=50)\n",
    "\n",
    "        # Unfreeze the upper layers of the convolutional base\n",
    "        for layer in model.layers[0].layers[15:]:\n",
    "            layer.trainable = True\n",
    "        \n",
    "        # Compile the model for fine-tuning\n",
    "        model.compile(optimizer=Adam(learning_rate=5e-6), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        # Entraîner le classifieur sur les données d'entraînement\n",
    "        print(f'Fold {i} fine tuning')\n",
    "        scores[i]['history_fine_tuning'] = model.fit(x=X_train,y=y_train,validation_data=(X_val,y_val),epochs=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.concatenate((history.history['accuracy'],history_fine_tuning.history['accuracy']),axis=0))\n",
    "plt.plot(np.concatenate((history.history['val_accuracy'],history_fine_tuning.history['val_accuracy']),axis=0))\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.concatenate((history.history['loss'],history_fine_tuning.history['loss']),axis=0))\n",
    "plt.plot(np.concatenate((history.history['val_loss'],history_fine_tuning.history['val_loss']),axis=0))\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'history': None, 'history_fine_tuning': None},\n",
       " 1: {'history': None, 'history_fine_tuning': None},\n",
       " 2: {'history': None, 'history_fine_tuning': None},\n",
       " 3: {'history': <keras.callbacks.History at 0x15008932bf0>,\n",
       "  'history_fine_tuning': <keras.callbacks.History at 0x15044bd61d0>},\n",
       " 4: {'history': None, 'history_fine_tuning': None}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('history_4.npy',scores[4]['history'].history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "history=np.load('history_4.npy',allow_pickle='TRUE').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.6076133847236633,\n",
       "  0.5119550228118896,\n",
       "  0.45512035489082336,\n",
       "  0.41340169310569763,\n",
       "  0.3831617534160614,\n",
       "  0.36029306054115295,\n",
       "  0.34224456548690796,\n",
       "  0.32872122526168823,\n",
       "  0.31772056221961975,\n",
       "  0.3082601726055145,\n",
       "  0.30121830105781555,\n",
       "  0.2946246862411499,\n",
       "  0.2893848419189453,\n",
       "  0.28565722703933716,\n",
       "  0.2804762125015259,\n",
       "  0.2774817943572998,\n",
       "  0.27415260672569275,\n",
       "  0.27118176221847534,\n",
       "  0.26922252774238586,\n",
       "  0.2672516703605652,\n",
       "  0.26537400484085083,\n",
       "  0.26316073536872864,\n",
       "  0.2615756094455719,\n",
       "  0.2598535418510437,\n",
       "  0.25900721549987793,\n",
       "  0.25737398862838745,\n",
       "  0.2561050355434418,\n",
       "  0.25433868169784546,\n",
       "  0.2543838620185852,\n",
       "  0.25266116857528687,\n",
       "  0.2518019378185272,\n",
       "  0.25076478719711304,\n",
       "  0.24994918704032898,\n",
       "  0.24908329546451569,\n",
       "  0.24815984070301056,\n",
       "  0.24749702215194702,\n",
       "  0.24651531875133514,\n",
       "  0.24589426815509796,\n",
       "  0.2457185834646225,\n",
       "  0.2443080097436905,\n",
       "  0.24430184066295624,\n",
       "  0.24321138858795166,\n",
       "  0.24224723875522614,\n",
       "  0.24308039247989655,\n",
       "  0.240918830037117,\n",
       "  0.2410796731710434,\n",
       "  0.23989419639110565,\n",
       "  0.23957374691963196,\n",
       "  0.23852457106113434,\n",
       "  0.2377665489912033],\n",
       " 'accuracy': [0.6279940009117126,\n",
       "  0.7722055912017822,\n",
       "  0.7697106003761292,\n",
       "  0.7786926031112671,\n",
       "  0.7994012236595154,\n",
       "  0.8063872456550598,\n",
       "  0.7981536984443665,\n",
       "  0.8066367506980896,\n",
       "  0.8033931851387024,\n",
       "  0.8141217827796936,\n",
       "  0.8068862557411194,\n",
       "  0.8068862557411194,\n",
       "  0.8091317415237427,\n",
       "  0.8073852062225342,\n",
       "  0.813622772693634,\n",
       "  0.8093812465667725,\n",
       "  0.8091317415237427,\n",
       "  0.8146207332611084,\n",
       "  0.8123752474784851,\n",
       "  0.8123752474784851,\n",
       "  0.8158682584762573,\n",
       "  0.8138722777366638,\n",
       "  0.8111277222633362,\n",
       "  0.8168662786483765,\n",
       "  0.8146207332611084,\n",
       "  0.8176147937774658,\n",
       "  0.8183632493019104,\n",
       "  0.8218562602996826,\n",
       "  0.8153692483901978,\n",
       "  0.8231037855148315,\n",
       "  0.8231037855148315,\n",
       "  0.8228542804718018,\n",
       "  0.8238523006439209,\n",
       "  0.8231037855148315,\n",
       "  0.8270958065986633,\n",
       "  0.8246008157730103,\n",
       "  0.8241018056869507,\n",
       "  0.8231037855148315,\n",
       "  0.8241018056869507,\n",
       "  0.8260977864265442,\n",
       "  0.8255987763404846,\n",
       "  0.8253493309020996,\n",
       "  0.8250998258590698,\n",
       "  0.8208582997322083,\n",
       "  0.8303393125534058,\n",
       "  0.8288423418998718,\n",
       "  0.830089807510376,\n",
       "  0.8265967965126038,\n",
       "  0.8313373327255249,\n",
       "  0.8310878276824951],\n",
       " 'val_loss': [0.5440174341201782,\n",
       "  0.47479119896888733,\n",
       "  0.42684242129325867,\n",
       "  0.38955116271972656,\n",
       "  0.3617115318775177,\n",
       "  0.3421071469783783,\n",
       "  0.32689157128334045,\n",
       "  0.31371641159057617,\n",
       "  0.3030789792537689,\n",
       "  0.2953970432281494,\n",
       "  0.2936747670173645,\n",
       "  0.28237563371658325,\n",
       "  0.29109498858451843,\n",
       "  0.2728497385978699,\n",
       "  0.26380136609077454,\n",
       "  0.2638351023197174,\n",
       "  0.25772082805633545,\n",
       "  0.25295332074165344,\n",
       "  0.2599523365497589,\n",
       "  0.2717471122741699,\n",
       "  0.25383061170578003,\n",
       "  0.2635158598423004,\n",
       "  0.2603825330734253,\n",
       "  0.2519243061542511,\n",
       "  0.2515394389629364,\n",
       "  0.24220812320709229,\n",
       "  0.24847380816936493,\n",
       "  0.23439376056194305,\n",
       "  0.2561500370502472,\n",
       "  0.24707110226154327,\n",
       "  0.2514215111732483,\n",
       "  0.24020302295684814,\n",
       "  0.24475421011447906,\n",
       "  0.2405320405960083,\n",
       "  0.24595946073532104,\n",
       "  0.2521668076515198,\n",
       "  0.23696574568748474,\n",
       "  0.25165244936943054,\n",
       "  0.2342626005411148,\n",
       "  0.24679012596607208,\n",
       "  0.24042510986328125,\n",
       "  0.24450957775115967,\n",
       "  0.22870506346225739,\n",
       "  0.24222016334533691,\n",
       "  0.2417372763156891,\n",
       "  0.23292215168476105,\n",
       "  0.23206160962581635,\n",
       "  0.23545831441879272,\n",
       "  0.2525162100791931,\n",
       "  0.26359325647354126],\n",
       " 'val_accuracy': [0.796407163143158,\n",
       "  0.7634730339050293,\n",
       "  0.727544903755188,\n",
       "  0.7594810128211975,\n",
       "  0.7704590559005737,\n",
       "  0.7614770531654358,\n",
       "  0.7614770531654358,\n",
       "  0.7694610953330994,\n",
       "  0.7704590559005737,\n",
       "  0.772455096244812,\n",
       "  0.7614770531654358,\n",
       "  0.7784430980682373,\n",
       "  0.7594810128211975,\n",
       "  0.7834331393241882,\n",
       "  0.7944111824035645,\n",
       "  0.78742516040802,\n",
       "  0.7934131622314453,\n",
       "  0.8093812465667725,\n",
       "  0.7834331393241882,\n",
       "  0.7654690742492676,\n",
       "  0.78742516040802,\n",
       "  0.7764471173286438,\n",
       "  0.7804391384124756,\n",
       "  0.7864271402359009,\n",
       "  0.7864271402359009,\n",
       "  0.802395224571228,\n",
       "  0.7914171814918518,\n",
       "  0.8253493309020996,\n",
       "  0.78143709897995,\n",
       "  0.7914171814918518,\n",
       "  0.7834331393241882,\n",
       "  0.8003991842269897,\n",
       "  0.7944111824035645,\n",
       "  0.8033931851387024,\n",
       "  0.7924151420593262,\n",
       "  0.7844311594963074,\n",
       "  0.8103792667388916,\n",
       "  0.7834331393241882,\n",
       "  0.8063872456550598,\n",
       "  0.7934131622314453,\n",
       "  0.8043912053108215,\n",
       "  0.7954092025756836,\n",
       "  0.8123752474784851,\n",
       "  0.8013972043991089,\n",
       "  0.802395224571228,\n",
       "  0.8083832263946533,\n",
       "  0.8063872456550598,\n",
       "  0.8073852062225342,\n",
       "  0.7864271402359009,\n",
       "  0.7744510769844055]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_master_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
